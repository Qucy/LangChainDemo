{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447204ba-a878-4d15-8352-d631c77b1df7",
   "metadata": {},
   "source": [
    "# ü¶úÔ∏èüîó 1 - LangChain Overview\n",
    "\n",
    "LangChain is a framework designed to empower developers to build applications powered by language models. While many applications may simply call language models via an API, LangChain goes beyond this by enabling applications to be data-aware and agentic. This means that LangChain enables language models to connect with other sources of data and interact with their environment, resulting in even more powerful and differentiated applications.\n",
    "<img src=\"./star-history-langchain.png\" alt=\"LangChain star history\" width=\"600\"/>\n",
    "\n",
    "\n",
    "### LangChain help us chain the LLM services with different data sources or enviornments\n",
    "<img src=\"./LangChain_Overview.png\" alt=\"LangChain Overview\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189913f-c44a-47b4-98bd-b310752f4d2d",
   "metadata": {},
   "source": [
    "# ü¶úÔ∏èüîó 2 - LangChain Components\n",
    "\n",
    "\n",
    "### LangChain provide 7 components (as of now)\n",
    "\n",
    "- #### Schema\n",
    "Schema covers the basic data types that are used throughout the LangChain, like Text, ChatMessages, Examples and Document.\n",
    "\n",
    "- #### Models \n",
    "This includes generic interface for all LLMs like OPENAI, HuggingFace, AzureOpenAI and common utilities for working with LLMs.\n",
    "\n",
    "- #### Prompts\n",
    "This includes prompt management, prompt optimization and etc.\n",
    "\n",
    "- #### Memory\n",
    "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "- #### Agents\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
    "\n",
    "- #### Indexes\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains. Like Document Loaders, Text Splitters, VectorStores, Retrievers.\n",
    "\n",
    "- #### Chains\n",
    "Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f10a9-0340-455b-aaba-d742b79cbf43",
   "metadata": {},
   "source": [
    "# ü¶úÔ∏èüîó 3 - LangChain Use Cases\n",
    "\n",
    "### Use case 1 - LLM service and langChain agent for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2805dc21-0969-49ea-a126-f71fc62a1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://qucy-openai-test.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv('SERPAPI_API_KEY')\n",
    "\n",
    "# create LLM\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad792f-23ce-4422-837f-995158d69e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning is a field of Artificial Intelligence (AI) that enables computers to learn from data without the need for explicit programming. It is a process of using algorithms to parse data and learn from it to make decisions or predictions about new data. Machine learning algorithms are used in a variety of applications, such as data mining, natural language processing, image recognition, and recommendation systems.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is Machine Learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90433e56-6487-4b5c-8c5c-43665c5250b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nToday is November 5th, 2020.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What date is it today ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a531e652-e4e9-4eb2-ac66-84e3772f53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try to find out the current date and any events that have happened on that day in history.\n",
      "Action: Search \n",
      "Action Input: \"What's the date today\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe date today is Wednesday, April 26, 2023. What is the date today in numbers? Depending where you reside and who's asking the date in numbers can be shown ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should check for any historical events that occured on this day.\n",
      "Action: Search\n",
      "Action Input: \"Events on April 26\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mRossini Leaves Italy ¬∑ 1859 Dan Sickles is acquitted of murder on grounds of temporary insanity - 1st time this defense used successfully in the US ¬∑ 1865 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Today is Wednesday, April 26, 2023, and on this day in history, Rossini Left Italy in 1859 and Dan Sickles was acquitted of murder on grounds of temporary insanity in 1865.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Today is Wednesday, April 26, 2023, and on this day in history, Rossini Left Italy in 1859 and Dan Sickles was acquitted of murder on grounds of temporary insanity in 1865.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only Google search tools\n",
    "tools = load_tools([\"serpapi\"])\n",
    "\n",
    "# init agent and set verbose to true to print out the \n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# run agent\n",
    "agent.run(\"What's the date today ? What great events have taken place today in history?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82b1c7-6a8e-419c-8fba-828d1244a0f2",
   "metadata": {},
   "source": [
    "## Avaiable agent type\n",
    "### zero-shot-react-description\n",
    "This agent uses the ReAct framework to determine which tool to use based solely on the tool‚Äôs description. Any number of tools can be provided. This agent requires that a description is provided for each tool.\n",
    "<img src=\"./ReAct.png\" alt=\"LangChain Overview\" width=\"800\"/>\n",
    "\n",
    "### react-docstore\n",
    "This agent uses the ReAct framework to interact with a docstore. Two tools must be provided: a Search tool and a Lookup tool (they must be named exactly as so). The Search tool should search for a document, while the Lookup tool should lookup a term in the most recently found document. This agent is equivalent to the original ReAct paper, specifically the Wikipedia example.\n",
    "\n",
    "### self-ask-with-search\n",
    "This agent utilizes a single tool that should be named Intermediate Answer. This tool should be able to lookup factual answers to questions. This agent is equivalent to the original self ask with search paper, where a Google search API was provided as the tool.\n",
    "\n",
    "### conversational-react-description\n",
    "This agent is designed to be used in conversational settings. The prompt is designed to make the agent helpful and conversational. It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b129d4d-dbe1-456f-815c-e0ca9d458799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the profit for each quarter and calculate the percentage increase or drop\n",
      "Action: Search\n",
      "Action Input: Tesla profit 2022 Q1 and 2023 Q1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTesla said net income fell 24% to $2.51 billion, or 73 cents a share, from $3.32 billion, or 95 cents a share, a year ago. related investing ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to look for more specific numbers\n",
      "Action: Search\n",
      "Action Input: Tesla profit 2022 Q1 and 2023 Q1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTesla said net income fell 24% to $2.51 billion, or 73 cents a share, from $3.32 billion, or 95 cents a share, a year ago. related investing ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to compare the two quarters\n",
      "Action: Calculator\n",
      "Action Input: (3.32-2.51)/2.51\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 0.3227091633466136\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert that number to a percentage\n",
      "Action: Calculator\n",
      "Action Input: 0.3227091633466136*100\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 32.27091633466136\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Tesla's profit in 2022 Q1 was $3.32 billion and in 2023 Q1 was $2.51 billion. This is a drop of 32.27% in 2023 Q1.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Tesla's profit in 2022 Q1 was $3.32 billion and in 2023 Q1 was $2.51 billion. This is a drop of 32.27% in 2023 Q1.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search tools and math tools\n",
    "tools = load_tools(['serpapi', 'llm-math', 'python_repl'], llm=llm)\n",
    "\n",
    "# init agent and set verbose to true to print out the \n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# run agent\n",
    "agent.run(\"What is Tesla's profit in 2022 Q1 and 2023 Q1 ? And how many percentage increase or drop in 2023 Q1 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b25c4e2-b997-4569-8f11-d36f71a334fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to write a sorting algorithm\n",
      "Action: Python REPL\n",
      "Action Input: def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to call the function to sort the list\n",
      "Action: Python REPL\n",
      "Action Input: print(quicksort([3,2,3,6,1,9,7,6,0]))\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m[0, 1, 2, 3, 3, 6, 6, 7, 9]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: [0, 1, 2, 3, 3, 6, 6, 7, 9]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[0, 1, 2, 3, 3, 6, 6, 7, 9]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run agent\n",
    "agent.run(\"Write quick sort function and sort list [3,2,3,6,1,9,7,6,0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742f005f-52cf-4c9e-bfe7-d58865ffa6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: list_tables_sql_db\n",
      "Action Input:\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3msqlite_sequence, users, prompt_template\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should check the schema of the most relevant table, prompt_template.\n",
      "Action: schema_sql_db\n",
      "Action Input: prompt_template\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE prompt_template (\n",
      "\tid INTEGER, \n",
      "\tname TEXT NOT NULL, \n",
      "\tdescription TEXT NOT NULL, \n",
      "\tsystem_prompt TEXT NOT NULL, \n",
      "\tuser_prompt TEXT NOT NULL, \n",
      "\tfew_shots BOOLEAN NOT NULL, \n",
      "\tcreated_at DATETIME NOT NULL, \n",
      "\tupdated_at DATETIME NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from prompt_template table:\n",
      "id\tname\tdescription\tsystem_prompt\tuser_prompt\tfew_shots\tcreated_at\tupdated_at\n",
      "1\tSummarization\tPrompt template for summarization\t{\"system\": \"I want you to be a summerization bot\"}\t{Summarize <{content}> in {target_language} in {content_length} words.}\tTrue\t2023-04-20 13:52:06\t2023-04-20 13:52:06\n",
      "2\tTranslation\tPrompt template for translation\t{\"system\": \"I want you to be a translation bot\"}\t{Translate <{content}> from {source_language} to {target_language}.}\tTrue\t2023-04-20 13:52:06\t2023-04-20 13:52:06\n",
      "3\tQuestion Answering\tPrompt template for question answering\t{\"system\": \"I want you to be a question answering bot\"}\t{Answer the question <{question}> in {target_language}.}\tTrue\t2023-04-20 13:52:06\t2023-04-20 13:52:06\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have the schema for the prompt_template table, now I should query for the number of records in the table.\n",
      "Action: query_sql_db\n",
      "Action Input: SELECT COUNT(*) AS total_records FROM prompt_template;\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(3,)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: There are 3 records in the prompt_template table and the columns are id, name, description, system_prompt, user_prompt, few_shots, created_at, and updated_at.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 3 records in the prompt_template table and the columns are id, name, description, system_prompt, user_prompt, few_shots, created_at, and updated_at.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:////mnt/disk1/python-projects/Chater-GPT-Backend/db/gpt-demo.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "# create agent\n",
    "agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\n",
    "\n",
    "# run agent\n",
    "agent_executor.run(\"What columns do prompt_template table have and how many records in the table ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7d2fd4-dbe7-4125-bc04-8a074277a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create large language model service\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\")\n",
    "\n",
    "# create DB tool\n",
    "db = SQLDatabase.from_uri(\"sqlite://///mnt/disk1/python-projects/LangChainDemo/demo.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "# create agent with DB tool\n",
    "agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9959f5af-4c4f-4689-9aa4-acff1123ea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: list_tables_sql_db\n",
      "Action Input: \"\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3msqlite_sequence, stocks, credit_card_transactions\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should query the schema of the stocks table.\n",
      "Action: schema_sql_db\n",
      "Action Input: \"stocks\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE stocks (\n",
      "\tid INTEGER, \n",
      "\tdate DATE NOT NULL, \n",
      "\tstock_symbol VARCHAR(10) NOT NULL, \n",
      "\tstock_name VARCHAR(255) NOT NULL, \n",
      "\tpb FLOAT, \n",
      "\tpe FLOAT, \n",
      "\tdividend_yield FLOAT, \n",
      "\tmarket_value FLOAT, \n",
      "\tturnover_ratio FLOAT, \n",
      "\thistorical_high FLOAT, \n",
      "\thistorical_low FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from stocks table:\n",
      "id\tdate\tstock_symbol\tstock_name\tpb\tpe\tdividend_yield\tmarket_value\tturnover_ratio\thistorical_high\thistorical_low\n",
      "1\t2022-01-01\tAAPL\tApple Inc.\t31.4500000000\t27.1100000000\t0.6300000000\t2500000000000.0000000000\t10.5000000000\t325.4700000000\t15.1200000000\n",
      "2\t2022-01-01\tGOOGL\tAlphabet Inc.\t5.3200000000\t29.2200000000\t0.3700000000\t1800000000000.0000000000\t8.2000000000\t2125.6700000000\t200.2500000000\n",
      "3\t2022-01-01\tAMZN\tAmazon.com Inc.\t20.1500000000\t69.5400000000\t0E-10\t1600000000000.0000000000\t12.3000000000\t3550.5000000000\t300.4800000000\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know what columns I need to query and what table.\n",
      "Action: query_checker_sql_db\n",
      "Action Input: \"SELECT stock_symbol, stock_name, pb FROM stocks ORDER BY pb DESC LIMIT 10;\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "\n",
      "SELECT stock_symbol, stock_name, pb FROM stocks ORDER BY pb DESC LIMIT 10;\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The query looks correct, I can go ahead and execute it.\n",
      "Action: query_sql_db\n",
      "Action Input: \"SELECT stock_symbol, stock_name, pb FROM stocks ORDER BY pb DESC LIMIT 10;\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[('AAPL', 'Apple Inc.', 31.45), ('AAPL', 'Apple Inc.', 30.45), ('AAPL', 'Apple Inc.', 29.45), ('TSLA', 'Tesla Inc.', 27.27), ('TSLA', 'Tesla Inc.', 26.27), ('TSLA', 'Tesla Inc.', 25.27), ('AMZN', 'Amazon.com Inc.', 21.15), ('AMZN', 'Amazon.com Inc.', 20.85), ('AMZN', 'Amazon.com Inc.', 20.15), ('MSFT', 'Microsoft Corporation', 14.58)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The stock with the highest PB is AAPL (Apple Inc.) with a PB of 31.45.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The stock with the highest PB is AAPL (Apple Inc.) with a PB of 31.45.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask question using natural languager\n",
    "agent_executor.run(\"Which stock has the highest PB ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd02e19e-816e-4c2a-a35e-62724aee88ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: list_tables_sql_db\n",
      "Action Input: \"\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3msqlite_sequence, stocks, credit_card_transactions\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should check the schema of the stocks table to get more information\n",
      "Action: schema_sql_db\n",
      "Action Input: \"stocks\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE stocks (\n",
      "\tid INTEGER, \n",
      "\tdate DATE NOT NULL, \n",
      "\tstock_symbol VARCHAR(10) NOT NULL, \n",
      "\tstock_name VARCHAR(255) NOT NULL, \n",
      "\tpb FLOAT, \n",
      "\tpe FLOAT, \n",
      "\tdividend_yield FLOAT, \n",
      "\tmarket_value FLOAT, \n",
      "\tturnover_ratio FLOAT, \n",
      "\thistorical_high FLOAT, \n",
      "\thistorical_low FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from stocks table:\n",
      "id\tdate\tstock_symbol\tstock_name\tpb\tpe\tdividend_yield\tmarket_value\tturnover_ratio\thistorical_high\thistorical_low\n",
      "1\t2022-01-01\tAAPL\tApple Inc.\t31.4500000000\t27.1100000000\t0.6300000000\t2500000000000.0000000000\t10.5000000000\t325.4700000000\t15.1200000000\n",
      "2\t2022-01-01\tGOOGL\tAlphabet Inc.\t5.3200000000\t29.2200000000\t0.3700000000\t1800000000000.0000000000\t8.2000000000\t2125.6700000000\t200.2500000000\n",
      "3\t2022-01-01\tAMZN\tAmazon.com Inc.\t20.1500000000\t69.5400000000\t0E-10\t1600000000000.0000000000\t12.3000000000\t3550.5000000000\t300.4800000000\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the schema, I should query the stocks table to find stocks with a low PE\n",
      "Action: query_sql_db\n",
      "Action Input: SELECT stock_symbol, stock_name, pe FROM stocks WHERE pe < 20 ORDER BY pe DESC LIMIT 10\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should check my query with query_checker_sql_db before executing it\n",
      "Action: query_checker_sql_db\n",
      "Action Input: SELECT stock_symbol, stock_name, pe FROM stocks WHERE pe < 20 ORDER BY pe DESC LIMIT 10\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "\n",
      "SELECT stock_symbol, stock_name, pe FROM stocks WHERE pe < 20 ORDER BY pe DESC LIMIT 10\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m My query is correct, I should execute it\n",
      "Action: query_sql_db\n",
      "Action Input: SELECT stock_symbol, stock_name, pe FROM stocks WHERE pe < 20 ORDER BY pe DESC LIMIT 10\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: There are no stocks with a PE value below 20.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are no stocks with a PE value below 20.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask question using natural language\n",
    "agent_executor.run(\"Which stock is recommended to hold when considering PE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0f7310-f898-4f50-925d-2d60655e1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: list_tables_sql_db\n",
      "Action Input: \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3msqlite_sequence, stocks, credit_card_transactions\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The table 'stocks' looks the most relevant. \n",
      "Action: schema_sql_db\n",
      "Action Input: stocks\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE stocks (\n",
      "\tid INTEGER, \n",
      "\tdate DATE NOT NULL, \n",
      "\tstock_symbol VARCHAR(10) NOT NULL, \n",
      "\tstock_name VARCHAR(255) NOT NULL, \n",
      "\tpb FLOAT, \n",
      "\tpe FLOAT, \n",
      "\tdividend_yield FLOAT, \n",
      "\tmarket_value FLOAT, \n",
      "\tturnover_ratio FLOAT, \n",
      "\thistorical_high FLOAT, \n",
      "\thistorical_low FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from stocks table:\n",
      "id\tdate\tstock_symbol\tstock_name\tpb\tpe\tdividend_yield\tmarket_value\tturnover_ratio\thistorical_high\thistorical_low\n",
      "1\t2022-01-01\tAAPL\tApple Inc.\t31.4500000000\t27.1100000000\t0.6300000000\t2500000000000.0000000000\t10.5000000000\t325.4700000000\t15.1200000000\n",
      "2\t2022-01-01\tGOOGL\tAlphabet Inc.\t5.3200000000\t29.2200000000\t0.3700000000\t1800000000000.0000000000\t8.2000000000\t2125.6700000000\t200.2500000000\n",
      "3\t2022-01-01\tAMZN\tAmazon.com Inc.\t20.1500000000\t69.5400000000\t0E-10\t1600000000000.0000000000\t12.3000000000\t3550.5000000000\t300.4800000000\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The PE for Apple Inc. is in the pe column. \n",
      "Action: query_sql_db\n",
      "Action Input: SELECT AVG(pe) FROM stocks WHERE stock_name = 'Apple Inc.'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(26.11,)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The average PE for Apple Inc. is 26.11.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average PE for Apple Inc. is 26.11.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask question using natural language\n",
    "agent_executor.run(\"What is the average PE for Apple Inc.?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f9ce2-f452-46ab-9531-c3862dd3dfa9",
   "metadata": {},
   "source": [
    "### Use case 2 - Document summarization\n",
    "\n",
    "<img src=\"./langChain_map_reduce.png\" alt=\"LangChain map reduce\" width=\"800\"/>\n",
    "<img src=\"./langChain_refine.png\" alt=\"LangChain map refine\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b190f4ea-bf3e-4114-a3d7-3d03bf614e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:1\n",
      "documents:271\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Published as a conference paper at ICLR 2023\n",
      "\n",
      "R E A C T : S YNERGIZING R EASONING L ANGUAGE M ODELS\n",
      "\n",
      "AND\n",
      "\n",
      "ACTING\n",
      "\n",
      "IN\n",
      "\n",
      "Shunyu Yao‚àó*,1 , Jeffrey Zhao2 , Dian Yu2 , Nan Du2 , Izhak Shafran2 , Karthik Narasimhan1 , Yuan Cao2 1\n",
      "\n",
      "arXiv:2210.03629v3 [cs.CL] 10 Mar 2023\n",
      "\n",
      "Department of Computer Science, Princeton University 2 Google Research, Brain team 1 {shunyuy,karthikn}@princeton.edu 2 {jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point:  \n",
      "This paper presents REACT, a model that synergizes reasoning language models and acting capabilities to bring about better decision-making. It was presented at ICLR 2023 and authored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao from Princeton University and Google Research Brain team.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "A BSTRACT While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "This paper presents REACT, a model that synergizes reasoning language models and acting capabilities to bring about better decision-making. It was presented at ICLR 2023 and authored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao from Princeton University and Google Research Brain team. While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, REACT explores the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two to bring about better decision-making.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA)\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "This paper presents REACT, a model that synergizes reasoning language models and acting capabilities to bring about better decision-making. It was presented at ICLR 2023 and authored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao from Princeton University and Google Research Brain team. While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, REACT explores the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two to bring about better decision-making. The traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively,\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "This paper presents REACT, a model that synergizes reasoning language models and acting capabilities to bring about better decision-making. It was presented at ICLR 2023 and authored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao from Princeton University and Google Research Brain team. While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, REACT explores the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two to bring about better decision-making. The traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks, demonstrating its effectiveness over state-of-the-art baselines. It\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "while being prompted with only one or two in-context examples.\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis paper presents REACT, a model that synergizes reasoning language models and acting capabilities to bring about better decision-making. It was presented at ICLR 2023 and authored by Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao from Princeton University and Google Research Brain team. While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, REACT explores the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two to bring about better decision-making. The traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments, while being prompted with only one or two in-context examples. We apply our approach, named ReAct, to a diverse set of language and decision making tasks, demonstrating its'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# import document\n",
    "loader = UnstructuredFileLoader(\"./ReAct.txt\")\n",
    "# load document\n",
    "document = loader.load()\n",
    "print(f'documents:{len(document)}')\n",
    "\n",
    "# split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(document)\n",
    "print(f'documents:{len(split_documents)}')\n",
    "\n",
    "\n",
    "# create summary chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "\n",
    "# show top 5 summarization\n",
    "chain.run(split_documents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff5d34-58ef-422a-8802-34217b0fb4ba",
   "metadata": {},
   "source": [
    "### Use case 3 - Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "506fde96-fcb4-41a3-8296-88ef413aa928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "One classic dish from Rome is Pasta alla Carbonara, a rich and creamy pasta dish made with eggs, guanciale (Italian cured pork cheek), pecorino cheese, black pepper, and olive oil.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Recipe for Pasta alla Carbonara:\n",
      "\n",
      "Ingredients:\n",
      "- 4 ounces guanciale, diced\n",
      "- 2 eggs\n",
      "- 1 cup grated Pecorino cheese\n",
      "- Olive oil\n",
      "- Black pepper\n",
      "- 1 pound of pasta\n",
      "\n",
      "Instructions:\n",
      "1. Bring a pot of salted water to a boil and cook the pasta according to the package instructions. Drain, reserving 1/2 cup of the cooking water. \n",
      "2. In a large skillet, heat 2 tablespoons of olive oil over medium-high heat. Add the guanciale and cook until golden, about 5 minutes. \n",
      "3. In a large bowl, whisk together the eggs, Pecorino cheese, and a generous amount of freshly ground black pepper. \n",
      "4. Add the cooked pasta to the skillet with the guanciale and stir to combine. \n",
      "5. Pour the egg mixture over the pasta and stir to combine. Add a few tablespoons of the reserved pasta cooking water, if needed. \n",
      "6. Cook until the sauce thickens and coats the pasta, about 1 minute.\n",
      "7. Serve with more Pecorino cheese and freshly ground black pepper. Enjoy!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# dish Chain\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "dish_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# recipe Chain\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "recipe_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Use SimpleSequentialChain to Chain the 2 different answers\n",
    "overall_chain = SimpleSequentialChain(chains=[dish_chain, recipe_chain], verbose=True)\n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bec380-e3cd-4842-8420-5d658e369db5",
   "metadata": {},
   "source": [
    "### Eample 4 - Structure output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47a9bb20-cf3d-4682-b8a0-f2d46d05bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"welcom to califonya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# define response schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# init output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# {\n",
    "#\t\"bad_string\": string  // This a poorly formatted user input string\n",
    "#\t\"good_string\": string  // This is your response, a reformatted response\n",
    "#}\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# define prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "llm_output = llm(promptValue)\n",
    "\n",
    "print(llm_output)\n",
    "\n",
    "# parse the output\n",
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6940610-bcbf-4a62-a2c8-2a19b012d0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
